{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMl40hAr17vYQNt0dDyUHXC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sb2539/AI-study/blob/master/broad_cycle_mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "id": "Gl2G3LMx3-nB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d4654d7-087a-4ed0-abdf-b691d0446357"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.7/dist-packages (0.6.7)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.13.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
        "from timm.models.layers import DropPath, trunc_normal_\n",
        "from timm.models.registry import register_model\n",
        "from timm.models.layers.helpers import to_2tuple\n",
        "\n",
        "import math\n",
        "from torch import Tensor\n",
        "from torch.nn import init\n",
        "from torch.nn.modules.utils import _pair\n",
        "from torchvision.ops.deform_conv import deform_conv2d as deform_conv2d_tv"
      ],
      "metadata": {
        "id": "-DiLW8Pc4JBj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "G8iswrAJ4Kbi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CycleFC(nn.Module):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size,  # re-defined kernel_size, represent the spatial area of staircase FC\n",
        "        stride: int = 1,\n",
        "        padding: int = 0,\n",
        "        dilation: int = 1,\n",
        "        groups: int = 1,\n",
        "        bias: bool = True,\n",
        "    ):\n",
        "        super(CycleFC, self).__init__()\n",
        "\n",
        "        if in_channels % groups != 0:\n",
        "            raise ValueError('in_channels must be divisible by groups')\n",
        "        if out_channels % groups != 0:\n",
        "            raise ValueError('out_channels must be divisible by groups')\n",
        "        if stride != 1:\n",
        "            raise ValueError('stride must be 1')\n",
        "        if padding != 0:\n",
        "            raise ValueError('padding must be 0')\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = _pair(stride)\n",
        "        self.padding = _pair(padding)\n",
        "        self.dilation = _pair(dilation)\n",
        "        self.groups = groups\n",
        "\n",
        "        self.weight = nn.Parameter(torch.empty(out_channels, in_channels // groups, 1, 1))  \n",
        "\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.empty(out_channels))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.register_buffer('offset', self.gen_offset())\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self) -> None:\n",
        "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "\n",
        "        if self.bias is not None:\n",
        "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
        "            bound = 1 / math.sqrt(fan_in)\n",
        "            init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "    def gen_offset(self):\n",
        "        \"\"\"\n",
        "        offset (Tensor[batch_size, 2 * offset_groups * kernel_height * kernel_width,\n",
        "            out_height, out_width]): offsets to be applied for each position in the\n",
        "            convolution kernel.\n",
        "        \"\"\"\n",
        "        offset = torch.empty(1, self.in_channels*2, 1, 1)\n",
        "        start_idx = (self.kernel_size[0] * self.kernel_size[1]) // 2\n",
        "        assert self.kernel_size[0] == 1 or self.kernel_size[1] == 1, self.kernel_size\n",
        "        for i in range(self.in_channels):\n",
        "            if self.kernel_size[0] == 1:\n",
        "                offset[0, 2 * i + 0, 0, 0] = 0\n",
        "                offset[0, 2 * i + 1, 0, 0] = (i + start_idx) % self.kernel_size[1] - (self.kernel_size[1] // 2)\n",
        "            else:\n",
        "                offset[0, 2 * i + 0, 0, 0] = (i + start_idx) % self.kernel_size[0] - (self.kernel_size[0] // 2)\n",
        "                offset[0, 2 * i + 1, 0, 0] = 0\n",
        "        return offset\n",
        "\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input (Tensor[batch_size, in_channels, in_height, in_width]): input tensor\n",
        "        \"\"\"\n",
        "        B, C, H, W = input.size()        \n",
        "        return deform_conv2d_tv(input, self.offset.expand(B, -1, H, W), self.weight, self.bias, stride=self.stride,\n",
        "                                padding=self.padding, dilation=self.dilation)\n",
        "    def extra_repr(self) -> str:\n",
        "        s = self.__class__.__name__ + '('\n",
        "        s += '{in_channels}'\n",
        "        s += ', {out_channels}'\n",
        "        s += ', kernel_size={kernel_size}'\n",
        "        s += ', stride={stride}'\n",
        "        s += ', padding={padding}' if self.padding != (0, 0) else ''\n",
        "        s += ', dilation={dilation}' if self.dilation != (1, 1) else ''\n",
        "        s += ', groups={groups}' if self.groups != 1 else ''\n",
        "        s += ', bias=False' if self.bias is None else ''\n",
        "        s += ')'\n",
        "        return s.format(**self.__dict__)"
      ],
      "metadata": {
        "id": "V0iZP3uW4bOh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CycleMLP(nn.Module):\n",
        "    def __init__(self, dim, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.mlp_c = nn.Linear(dim, dim, bias=qkv_bias)\n",
        "\n",
        "        self.sfc_h = CycleFC(dim, dim, (1, 3), 1, 0)\n",
        "        self.sfc_w = CycleFC(dim, dim, (3, 1), 1, 0)\n",
        "\n",
        "        self.reweight = Mlp(dim, dim // 4, dim * 3)\n",
        "\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, H, W, C = x.shape\n",
        "        h = self.sfc_h(x.permute(0, 3, 1, 2)).permute(0, 2, 3, 1)\n",
        "        w = self.sfc_w(x.permute(0, 3, 1, 2)).permute(0, 2, 3, 1)\n",
        "        c = self.mlp_c(x)\n",
        "        a = (h + w + c).permute(0, 3, 1, 2).flatten(2).mean(2)\n",
        "        a = self.reweight(a).reshape(B, C, 3).permute(2, 0, 1).softmax(dim=0).unsqueeze(2).unsqueeze(2)\n",
        "\n",
        "        x = h * a[0] + w * a[1] + c * a[2]\n",
        "\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "NWUVrdmH9Gou"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CycleBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, skip_lam=1.0, mlp_fn=CycleMLP):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = mlp_fn(dim, qkv_bias=qkv_bias, qk_scale=None, attn_drop=attn_drop)\n",
        "\n",
        "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer)\n",
        "        self.skip_lam = skip_lam\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path(self.attn(self.norm1(x))) / self.skip_lam\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x))) / self.skip_lam\n",
        "        return x"
      ],
      "metadata": {
        "id": "6UvDPshl9KVK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedOverlapping(nn.Module):\n",
        "    \"\"\" 2D Image to Patch Embedding with overlapping\n",
        "    \"\"\"\n",
        "    def __init__(self, patch_size=16, stride=16, padding=0, in_chans=3, embed_dim=768, norm_layer=None, groups=1):\n",
        "        super().__init__()\n",
        "        patch_size = to_2tuple(patch_size)\n",
        "        stride = to_2tuple(stride)\n",
        "        padding = to_2tuple(padding)\n",
        "        self.patch_size = patch_size\n",
        "        # remove image_size in model init to support dynamic image size\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride, padding=padding, groups=groups)\n",
        "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "tK-v_Ouc9MP_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Downsample(nn.Module):\n",
        "    \"\"\" Downsample transition stage\n",
        "    \"\"\"\n",
        "    def __init__(self, in_embed_dim, out_embed_dim, patch_size):\n",
        "        super().__init__()\n",
        "        assert patch_size == 2, patch_size\n",
        "        self.proj = nn.Conv2d(in_embed_dim, out_embed_dim, kernel_size=(3, 3), stride=(2, 2), padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        x = self.proj(x)  # B, C, H, W\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "oLCmgnzu9ODG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def basic_blocks(dim, index, layers, mlp_ratio=3., qkv_bias=False, qk_scale=None, attn_drop=0.,\n",
        "                 drop_path_rate=0., skip_lam=1.0, mlp_fn=CycleMLP, **kwargs):\n",
        "    blocks = []\n",
        "\n",
        "    for block_idx in range(layers[index]):\n",
        "        block_dpr = drop_path_rate * (block_idx + sum(layers[:index])) / (sum(layers) - 1)\n",
        "        blocks.append(CycleBlock(dim, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                      attn_drop=attn_drop, drop_path=block_dpr, skip_lam=skip_lam, mlp_fn=mlp_fn))\n",
        "    blocks = nn.Sequential(*blocks)\n",
        "\n",
        "    return blocks"
      ],
      "metadata": {
        "id": "pz8MDAnD9PuH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FactorizedReduce(nn.Module):\n",
        "\n",
        "  def __init__(self, C_in, C_out, affine=True):\n",
        "    super(FactorizedReduce, self).__init__()\n",
        "    assert C_out % 2 == 0\n",
        "    self.relu = nn.ReLU(inplace=False)\n",
        "    self.conv_1 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)\n",
        "    self.conv_2 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False) \n",
        "    self.bn = nn.BatchNorm2d(C_out, affine=affine)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.relu(x)\n",
        "    out = torch.cat([self.conv_1(x), self.conv_2(x[:,:,1:,1:])], dim=1)\n",
        "    out = self.bn(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "0WpZZi269ltF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CycleNet(nn.Module):\n",
        "    \"\"\" CycleMLP Network \"\"\"\n",
        "    def __init__(self, layers, img_size=224, patch_size=4, in_chans=3, num_classes=1000,\n",
        "        embed_dims=None, transitions=None, segment_dim=None, mlp_ratios=None, skip_lam=1.0,\n",
        "        qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.,\n",
        "        norm_layer=nn.LayerNorm, mlp_fn=CycleMLP, fork_feat=False):\n",
        "\n",
        "        super().__init__()\n",
        "        if not fork_feat:\n",
        "            self.num_classes = num_classes\n",
        "        self.fork_feat = fork_feat\n",
        "\n",
        "        self.patch_embed = PatchEmbedOverlapping(patch_size=7, stride=4, padding=2, in_chans=3, embed_dim=embed_dims[0])\n",
        "       # 추가\n",
        "        self.reduction_stage_output = nn.ModuleList()\n",
        "       ###\n",
        "        network = []\n",
        "        stage_num = []\n",
        "        for i in range(len(layers)):\n",
        "            stage = basic_blocks(embed_dims[i], i, layers, mlp_ratio=mlp_ratios[i], qkv_bias=qkv_bias,\n",
        "                                 qk_scale=qk_scale, attn_drop=attn_drop_rate, drop_path_rate=drop_path_rate,\n",
        "                                 norm_layer=norm_layer, skip_lam=skip_lam, mlp_fn=mlp_fn)\n",
        "            network.append(stage)\n",
        "            stage_num.append(stage)\n",
        "            # 추가\n",
        "            if len(stage_num) > 1:\n",
        "                for j in range(len(stage_num)-1):\n",
        "                    reduction_out = FactorizedReduce(embed_dims[j], embed_dims[j])\n",
        "                    self.reduction_stage_output.append(reduction_out)\n",
        "            # \n",
        "            if i >= len(layers) - 1:\n",
        "                break\n",
        "            # 변경이 필요해 보이는 부분 \n",
        "            if transitions[i] or embed_dims[i] != embed_dims[i+1]:\n",
        "                patch_size = 2 if transitions[i] else 1\n",
        "                network.append(Downsample(embed_dims[i], embed_dims[i+1], patch_size))\n",
        "\n",
        "        self.network = nn.ModuleList(network)\n",
        "\n",
        "        if self.fork_feat:\n",
        "            # add a norm layer for each output\n",
        "            self.out_indices = [0, 2, 4, 6]\n",
        "            for i_emb, i_layer in enumerate(self.out_indices):\n",
        "                if i_emb == 0 and os.environ.get('FORK_LAST3', None):\n",
        "                    # TODO: more elegant way\n",
        "                    \"\"\"For RetinaNet, `start_level=1`. The first norm layer will not used.\n",
        "                    cmd: `FORK_LAST3=1 python -m torch.distributed.launch ...`\n",
        "                    \"\"\"\n",
        "                    layer = nn.Identity()\n",
        "                else:\n",
        "                    layer = norm_layer(embed_dims[i_emb])\n",
        "                layer_name = f'norm{i_layer}'\n",
        "                self.add_module(layer_name, layer)\n",
        "        else:\n",
        "            # Classifier head\n",
        "            final_dims = 0\n",
        "            for a in range(len(embed_dims)):\n",
        "                final_dims += embed_dims[a]\n",
        "            self.norm = norm_layer(final_dims)\n",
        "            self.head = nn.Linear(final_dims, num_classes) if num_classes > 0 else nn.Identity()\n",
        "        self.apply(self.cls_init_weights)\n",
        "\n",
        "    def cls_init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, CycleFC):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    #def init_weights(self, pretrained=None):\n",
        "     #   \"\"\" mmseg or mmdet `init_weight` \"\"\"\n",
        "      #  if isinstance(pretrained, str):\n",
        "       #     logger = get_root_logger()\n",
        "        #    load_checkpoint(self, pretrained, map_location='cpu', strict=False, logger=logger)\n",
        "\n",
        "    def get_classifier(self):\n",
        "        return self.head\n",
        "\n",
        "    def reset_classifier(self, num_classes, global_pool=''):\n",
        "        self.num_classes = num_classes\n",
        "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "    def forward_embeddings(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        # B,C,H,W-> B,H,W,C\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        return x\n",
        "\n",
        "    def forward_tokens(self, x):\n",
        "        outs = []\n",
        "        # 추가된 부분\n",
        "        stage_outputs = []\n",
        "        reduce_time = 0\n",
        "        stage_index = 0\n",
        "        ####\n",
        "        for idx, block in enumerate(self.network):\n",
        "            x = block(x)\n",
        "            print(idx)\n",
        "            # 추가됐지만 변경이 필요해 보이는 부분\n",
        "            if idx == stage_index :\n",
        "                stage_outputs.append(x)\n",
        "                stage_index +=2\n",
        "                if len(stage_outputs) > 1:\n",
        "                    for k in range(len(stage_outputs)-1):\n",
        "                        stage_outputs[k] = self.reduction_stage_output[reduce_time](stage_outputs[k].permute(0, 3, 1, 2)).permute(0, 2, 3, 1)\n",
        "                        reduce_time += 1\n",
        "                \"\"\"if idx+2 == 6\n",
        "                    for k in range(len(stage_outputs)):\n",
        "                        stage_outputs[k] = self.reduction_stage_output[reduce_time](stage_outputs[k])\n",
        "                        reduce_time += 1\"\"\"\n",
        "            #####\n",
        "            if self.fork_feat and idx in self.out_indices:\n",
        "                norm_layer = getattr(self, f'norm{idx}')\n",
        "                x_out = norm_layer(x)\n",
        "                outs.append(x_out.permute(0, 3, 1, 2).contiguous())\n",
        "        if self.fork_feat:\n",
        "            return outs\n",
        "        outs = []\n",
        "        for out in stage_outputs:\n",
        "            outs.append(out)\n",
        "        final_out = torch.cat(outs, dim = 3)\n",
        "        B, H, W, C = final_out.shape\n",
        "        x = final_out.reshape(B, -1, C)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_embeddings(x)\n",
        "        # B, H, W, C -> B, N, C\n",
        "        x = self.forward_tokens(x)\n",
        "        if self.fork_feat:\n",
        "            return x\n",
        "\n",
        "        x = self.norm(x)\n",
        "        cls_out = self.head(x.mean(1))\n",
        "        return cls_out"
      ],
      "metadata": {
        "id": "knwiM5Iq9Spa"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CycleMLP_B5(pretrained=False, **kwargs):\n",
        "    transitions = [True, True, True, True]\n",
        "    layers = [3, 4, 24, 3]\n",
        "    mlp_ratios = [4, 4, 4, 4]\n",
        "    embed_dims = [96, 192, 384, 768]\n",
        "    model = CycleNet(layers, embed_dims=embed_dims, patch_size=7, transitions=transitions,\n",
        "                     mlp_ratios=mlp_ratios, mlp_fn=CycleMLP, **kwargs)\n",
        "    model.default_cfg = default_cfgs['cycle_L']\n",
        "    return model"
      ],
      "metadata": {
        "id": "8IG1xC9f9Yor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    from torchsummary import summary\n",
        "    import pdb\n",
        "\n",
        "    transitions = [True, True, True, True]\n",
        "    layers = [3, 4, 24, 3]\n",
        "    mlp_ratios = [4, 4, 4, 4]\n",
        "    embed_dims = [96, 192, 384, 768]\n",
        "    model = CycleNet(layers, embed_dims=embed_dims, patch_size=7, transitions=transitions,\n",
        "                     mlp_ratios=mlp_ratios, mlp_fn=CycleMLP)\n",
        "    summary(model, (3,224,224), device = 'cpu')"
      ],
      "metadata": {
        "id": "x5SOWoXUHgDt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58f886e4-ceab-4d92-ead7-4e8b0f6cdf9b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 96, 56, 56]          14,208\n",
            "PatchEmbedOverlapping-2           [-1, 96, 56, 56]               0\n",
            "         LayerNorm-3           [-1, 56, 56, 96]             192\n",
            "           CycleFC-4           [-1, 96, 56, 56]           9,312\n",
            "           CycleFC-5           [-1, 96, 56, 56]           9,312\n",
            "            Linear-6           [-1, 56, 56, 96]           9,216\n",
            "            Linear-7                   [-1, 24]           2,328\n",
            "              GELU-8                   [-1, 24]               0\n",
            "           Dropout-9                   [-1, 24]               0\n",
            "           Linear-10                  [-1, 288]           7,200\n",
            "          Dropout-11                  [-1, 288]               0\n",
            "              Mlp-12                  [-1, 288]               0\n",
            "           Linear-13           [-1, 56, 56, 96]           9,312\n",
            "          Dropout-14           [-1, 56, 56, 96]               0\n",
            "         CycleMLP-15           [-1, 56, 56, 96]               0\n",
            "         Identity-16           [-1, 56, 56, 96]               0\n",
            "        LayerNorm-17           [-1, 56, 56, 96]             192\n",
            "           Linear-18          [-1, 56, 56, 384]          37,248\n",
            "             GELU-19          [-1, 56, 56, 384]               0\n",
            "          Dropout-20          [-1, 56, 56, 384]               0\n",
            "           Linear-21           [-1, 56, 56, 96]          36,960\n",
            "          Dropout-22           [-1, 56, 56, 96]               0\n",
            "              Mlp-23           [-1, 56, 56, 96]               0\n",
            "         Identity-24           [-1, 56, 56, 96]               0\n",
            "       CycleBlock-25           [-1, 56, 56, 96]               0\n",
            "        LayerNorm-26           [-1, 56, 56, 96]             192\n",
            "          CycleFC-27           [-1, 96, 56, 56]           9,312\n",
            "          CycleFC-28           [-1, 96, 56, 56]           9,312\n",
            "           Linear-29           [-1, 56, 56, 96]           9,216\n",
            "           Linear-30                   [-1, 24]           2,328\n",
            "             GELU-31                   [-1, 24]               0\n",
            "          Dropout-32                   [-1, 24]               0\n",
            "           Linear-33                  [-1, 288]           7,200\n",
            "          Dropout-34                  [-1, 288]               0\n",
            "              Mlp-35                  [-1, 288]               0\n",
            "           Linear-36           [-1, 56, 56, 96]           9,312\n",
            "          Dropout-37           [-1, 56, 56, 96]               0\n",
            "         CycleMLP-38           [-1, 56, 56, 96]               0\n",
            "         Identity-39           [-1, 56, 56, 96]               0\n",
            "        LayerNorm-40           [-1, 56, 56, 96]             192\n",
            "           Linear-41          [-1, 56, 56, 384]          37,248\n",
            "             GELU-42          [-1, 56, 56, 384]               0\n",
            "          Dropout-43          [-1, 56, 56, 384]               0\n",
            "           Linear-44           [-1, 56, 56, 96]          36,960\n",
            "          Dropout-45           [-1, 56, 56, 96]               0\n",
            "              Mlp-46           [-1, 56, 56, 96]               0\n",
            "         Identity-47           [-1, 56, 56, 96]               0\n",
            "       CycleBlock-48           [-1, 56, 56, 96]               0\n",
            "        LayerNorm-49           [-1, 56, 56, 96]             192\n",
            "          CycleFC-50           [-1, 96, 56, 56]           9,312\n",
            "          CycleFC-51           [-1, 96, 56, 56]           9,312\n",
            "           Linear-52           [-1, 56, 56, 96]           9,216\n",
            "           Linear-53                   [-1, 24]           2,328\n",
            "             GELU-54                   [-1, 24]               0\n",
            "          Dropout-55                   [-1, 24]               0\n",
            "           Linear-56                  [-1, 288]           7,200\n",
            "          Dropout-57                  [-1, 288]               0\n",
            "              Mlp-58                  [-1, 288]               0\n",
            "           Linear-59           [-1, 56, 56, 96]           9,312\n",
            "          Dropout-60           [-1, 56, 56, 96]               0\n",
            "         CycleMLP-61           [-1, 56, 56, 96]               0\n",
            "         Identity-62           [-1, 56, 56, 96]               0\n",
            "        LayerNorm-63           [-1, 56, 56, 96]             192\n",
            "           Linear-64          [-1, 56, 56, 384]          37,248\n",
            "             GELU-65          [-1, 56, 56, 384]               0\n",
            "          Dropout-66          [-1, 56, 56, 384]               0\n",
            "           Linear-67           [-1, 56, 56, 96]          36,960\n",
            "          Dropout-68           [-1, 56, 56, 96]               0\n",
            "              Mlp-69           [-1, 56, 56, 96]               0\n",
            "         Identity-70           [-1, 56, 56, 96]               0\n",
            "       CycleBlock-71           [-1, 56, 56, 96]               0\n",
            "           Conv2d-72          [-1, 192, 28, 28]         166,080\n",
            "       Downsample-73          [-1, 28, 28, 192]               0\n",
            "        LayerNorm-74          [-1, 28, 28, 192]             384\n",
            "          CycleFC-75          [-1, 192, 28, 28]          37,056\n",
            "          CycleFC-76          [-1, 192, 28, 28]          37,056\n",
            "           Linear-77          [-1, 28, 28, 192]          36,864\n",
            "           Linear-78                   [-1, 48]           9,264\n",
            "             GELU-79                   [-1, 48]               0\n",
            "          Dropout-80                   [-1, 48]               0\n",
            "           Linear-81                  [-1, 576]          28,224\n",
            "          Dropout-82                  [-1, 576]               0\n",
            "              Mlp-83                  [-1, 576]               0\n",
            "           Linear-84          [-1, 28, 28, 192]          37,056\n",
            "          Dropout-85          [-1, 28, 28, 192]               0\n",
            "         CycleMLP-86          [-1, 28, 28, 192]               0\n",
            "         Identity-87          [-1, 28, 28, 192]               0\n",
            "        LayerNorm-88          [-1, 28, 28, 192]             384\n",
            "           Linear-89          [-1, 28, 28, 768]         148,224\n",
            "             GELU-90          [-1, 28, 28, 768]               0\n",
            "          Dropout-91          [-1, 28, 28, 768]               0\n",
            "           Linear-92          [-1, 28, 28, 192]         147,648\n",
            "          Dropout-93          [-1, 28, 28, 192]               0\n",
            "              Mlp-94          [-1, 28, 28, 192]               0\n",
            "         Identity-95          [-1, 28, 28, 192]               0\n",
            "       CycleBlock-96          [-1, 28, 28, 192]               0\n",
            "        LayerNorm-97          [-1, 28, 28, 192]             384\n",
            "          CycleFC-98          [-1, 192, 28, 28]          37,056\n",
            "          CycleFC-99          [-1, 192, 28, 28]          37,056\n",
            "          Linear-100          [-1, 28, 28, 192]          36,864\n",
            "          Linear-101                   [-1, 48]           9,264\n",
            "            GELU-102                   [-1, 48]               0\n",
            "         Dropout-103                   [-1, 48]               0\n",
            "          Linear-104                  [-1, 576]          28,224\n",
            "         Dropout-105                  [-1, 576]               0\n",
            "             Mlp-106                  [-1, 576]               0\n",
            "          Linear-107          [-1, 28, 28, 192]          37,056\n",
            "         Dropout-108          [-1, 28, 28, 192]               0\n",
            "        CycleMLP-109          [-1, 28, 28, 192]               0\n",
            "        Identity-110          [-1, 28, 28, 192]               0\n",
            "       LayerNorm-111          [-1, 28, 28, 192]             384\n",
            "          Linear-112          [-1, 28, 28, 768]         148,224\n",
            "            GELU-113          [-1, 28, 28, 768]               0\n",
            "         Dropout-114          [-1, 28, 28, 768]               0\n",
            "          Linear-115          [-1, 28, 28, 192]         147,648\n",
            "         Dropout-116          [-1, 28, 28, 192]               0\n",
            "             Mlp-117          [-1, 28, 28, 192]               0\n",
            "        Identity-118          [-1, 28, 28, 192]               0\n",
            "      CycleBlock-119          [-1, 28, 28, 192]               0\n",
            "       LayerNorm-120          [-1, 28, 28, 192]             384\n",
            "         CycleFC-121          [-1, 192, 28, 28]          37,056\n",
            "         CycleFC-122          [-1, 192, 28, 28]          37,056\n",
            "          Linear-123          [-1, 28, 28, 192]          36,864\n",
            "          Linear-124                   [-1, 48]           9,264\n",
            "            GELU-125                   [-1, 48]               0\n",
            "         Dropout-126                   [-1, 48]               0\n",
            "          Linear-127                  [-1, 576]          28,224\n",
            "         Dropout-128                  [-1, 576]               0\n",
            "             Mlp-129                  [-1, 576]               0\n",
            "          Linear-130          [-1, 28, 28, 192]          37,056\n",
            "         Dropout-131          [-1, 28, 28, 192]               0\n",
            "        CycleMLP-132          [-1, 28, 28, 192]               0\n",
            "        Identity-133          [-1, 28, 28, 192]               0\n",
            "       LayerNorm-134          [-1, 28, 28, 192]             384\n",
            "          Linear-135          [-1, 28, 28, 768]         148,224\n",
            "            GELU-136          [-1, 28, 28, 768]               0\n",
            "         Dropout-137          [-1, 28, 28, 768]               0\n",
            "          Linear-138          [-1, 28, 28, 192]         147,648\n",
            "         Dropout-139          [-1, 28, 28, 192]               0\n",
            "             Mlp-140          [-1, 28, 28, 192]               0\n",
            "        Identity-141          [-1, 28, 28, 192]               0\n",
            "      CycleBlock-142          [-1, 28, 28, 192]               0\n",
            "       LayerNorm-143          [-1, 28, 28, 192]             384\n",
            "         CycleFC-144          [-1, 192, 28, 28]          37,056\n",
            "         CycleFC-145          [-1, 192, 28, 28]          37,056\n",
            "          Linear-146          [-1, 28, 28, 192]          36,864\n",
            "          Linear-147                   [-1, 48]           9,264\n",
            "            GELU-148                   [-1, 48]               0\n",
            "         Dropout-149                   [-1, 48]               0\n",
            "          Linear-150                  [-1, 576]          28,224\n",
            "         Dropout-151                  [-1, 576]               0\n",
            "             Mlp-152                  [-1, 576]               0\n",
            "          Linear-153          [-1, 28, 28, 192]          37,056\n",
            "         Dropout-154          [-1, 28, 28, 192]               0\n",
            "        CycleMLP-155          [-1, 28, 28, 192]               0\n",
            "        Identity-156          [-1, 28, 28, 192]               0\n",
            "       LayerNorm-157          [-1, 28, 28, 192]             384\n",
            "          Linear-158          [-1, 28, 28, 768]         148,224\n",
            "            GELU-159          [-1, 28, 28, 768]               0\n",
            "         Dropout-160          [-1, 28, 28, 768]               0\n",
            "          Linear-161          [-1, 28, 28, 192]         147,648\n",
            "         Dropout-162          [-1, 28, 28, 192]               0\n",
            "             Mlp-163          [-1, 28, 28, 192]               0\n",
            "        Identity-164          [-1, 28, 28, 192]               0\n",
            "      CycleBlock-165          [-1, 28, 28, 192]               0\n",
            "            ReLU-166           [-1, 96, 56, 56]               0\n",
            "          Conv2d-167           [-1, 48, 28, 28]           4,608\n",
            "          Conv2d-168           [-1, 48, 28, 28]           4,608\n",
            "     BatchNorm2d-169           [-1, 96, 28, 28]             192\n",
            "FactorizedReduce-170           [-1, 96, 28, 28]               0\n",
            "          Conv2d-171          [-1, 384, 14, 14]         663,936\n",
            "      Downsample-172          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-173          [-1, 14, 14, 384]             768\n",
            "         CycleFC-174          [-1, 384, 14, 14]         147,840\n",
            "         CycleFC-175          [-1, 384, 14, 14]         147,840\n",
            "          Linear-176          [-1, 14, 14, 384]         147,456\n",
            "          Linear-177                   [-1, 96]          36,960\n",
            "            GELU-178                   [-1, 96]               0\n",
            "         Dropout-179                   [-1, 96]               0\n",
            "          Linear-180                 [-1, 1152]         111,744\n",
            "         Dropout-181                 [-1, 1152]               0\n",
            "             Mlp-182                 [-1, 1152]               0\n",
            "          Linear-183          [-1, 14, 14, 384]         147,840\n",
            "         Dropout-184          [-1, 14, 14, 384]               0\n",
            "        CycleMLP-185          [-1, 14, 14, 384]               0\n",
            "        Identity-186          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-187          [-1, 14, 14, 384]             768\n",
            "          Linear-188         [-1, 14, 14, 1536]         591,360\n",
            "            GELU-189         [-1, 14, 14, 1536]               0\n",
            "         Dropout-190         [-1, 14, 14, 1536]               0\n",
            "          Linear-191          [-1, 14, 14, 384]         590,208\n",
            "         Dropout-192          [-1, 14, 14, 384]               0\n",
            "             Mlp-193          [-1, 14, 14, 384]               0\n",
            "        Identity-194          [-1, 14, 14, 384]               0\n",
            "      CycleBlock-195          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-196          [-1, 14, 14, 384]             768\n",
            "         CycleFC-197          [-1, 384, 14, 14]         147,840\n",
            "         CycleFC-198          [-1, 384, 14, 14]         147,840\n",
            "          Linear-199          [-1, 14, 14, 384]         147,456\n",
            "          Linear-200                   [-1, 96]          36,960\n",
            "            GELU-201                   [-1, 96]               0\n",
            "         Dropout-202                   [-1, 96]               0\n",
            "          Linear-203                 [-1, 1152]         111,744\n",
            "         Dropout-204                 [-1, 1152]               0\n",
            "             Mlp-205                 [-1, 1152]               0\n",
            "          Linear-206          [-1, 14, 14, 384]         147,840\n",
            "         Dropout-207          [-1, 14, 14, 384]               0\n",
            "        CycleMLP-208          [-1, 14, 14, 384]               0\n",
            "        Identity-209          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-210          [-1, 14, 14, 384]             768\n",
            "          Linear-211         [-1, 14, 14, 1536]         591,360\n",
            "            GELU-212         [-1, 14, 14, 1536]               0\n",
            "         Dropout-213         [-1, 14, 14, 1536]               0\n",
            "          Linear-214          [-1, 14, 14, 384]         590,208\n",
            "         Dropout-215          [-1, 14, 14, 384]               0\n",
            "             Mlp-216          [-1, 14, 14, 384]               0\n",
            "        Identity-217          [-1, 14, 14, 384]               0\n",
            "      CycleBlock-218          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-219          [-1, 14, 14, 384]             768\n",
            "         CycleFC-220          [-1, 384, 14, 14]         147,840\n",
            "         CycleFC-221          [-1, 384, 14, 14]         147,840\n",
            "          Linear-222          [-1, 14, 14, 384]         147,456\n",
            "          Linear-223                   [-1, 96]          36,960\n",
            "            GELU-224                   [-1, 96]               0\n",
            "         Dropout-225                   [-1, 96]               0\n",
            "          Linear-226                 [-1, 1152]         111,744\n",
            "         Dropout-227                 [-1, 1152]               0\n",
            "             Mlp-228                 [-1, 1152]               0\n",
            "          Linear-229          [-1, 14, 14, 384]         147,840\n",
            "         Dropout-230          [-1, 14, 14, 384]               0\n",
            "        CycleMLP-231          [-1, 14, 14, 384]               0\n",
            "        Identity-232          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-233          [-1, 14, 14, 384]             768\n",
            "          Linear-234         [-1, 14, 14, 1536]         591,360\n",
            "            GELU-235         [-1, 14, 14, 1536]               0\n",
            "         Dropout-236         [-1, 14, 14, 1536]               0\n",
            "          Linear-237          [-1, 14, 14, 384]         590,208\n",
            "         Dropout-238          [-1, 14, 14, 384]               0\n",
            "             Mlp-239          [-1, 14, 14, 384]               0\n",
            "        Identity-240          [-1, 14, 14, 384]               0\n",
            "      CycleBlock-241          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-242          [-1, 14, 14, 384]             768\n",
            "         CycleFC-243          [-1, 384, 14, 14]         147,840\n",
            "         CycleFC-244          [-1, 384, 14, 14]         147,840\n",
            "          Linear-245          [-1, 14, 14, 384]         147,456\n",
            "          Linear-246                   [-1, 96]          36,960\n",
            "            GELU-247                   [-1, 96]               0\n",
            "         Dropout-248                   [-1, 96]               0\n",
            "          Linear-249                 [-1, 1152]         111,744\n",
            "         Dropout-250                 [-1, 1152]               0\n",
            "             Mlp-251                 [-1, 1152]               0\n",
            "          Linear-252          [-1, 14, 14, 384]         147,840\n",
            "         Dropout-253          [-1, 14, 14, 384]               0\n",
            "        CycleMLP-254          [-1, 14, 14, 384]               0\n",
            "        Identity-255          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-256          [-1, 14, 14, 384]             768\n",
            "          Linear-257         [-1, 14, 14, 1536]         591,360\n",
            "            GELU-258         [-1, 14, 14, 1536]               0\n",
            "         Dropout-259         [-1, 14, 14, 1536]               0\n",
            "          Linear-260          [-1, 14, 14, 384]         590,208\n",
            "         Dropout-261          [-1, 14, 14, 384]               0\n",
            "             Mlp-262          [-1, 14, 14, 384]               0\n",
            "        Identity-263          [-1, 14, 14, 384]               0\n",
            "      CycleBlock-264          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-265          [-1, 14, 14, 384]             768\n",
            "         CycleFC-266          [-1, 384, 14, 14]         147,840\n",
            "         CycleFC-267          [-1, 384, 14, 14]         147,840\n",
            "          Linear-268          [-1, 14, 14, 384]         147,456\n",
            "          Linear-269                   [-1, 96]          36,960\n",
            "            GELU-270                   [-1, 96]               0\n",
            "         Dropout-271                   [-1, 96]               0\n",
            "          Linear-272                 [-1, 1152]         111,744\n",
            "         Dropout-273                 [-1, 1152]               0\n",
            "             Mlp-274                 [-1, 1152]               0\n",
            "          Linear-275          [-1, 14, 14, 384]         147,840\n",
            "         Dropout-276          [-1, 14, 14, 384]               0\n",
            "        CycleMLP-277          [-1, 14, 14, 384]               0\n",
            "        Identity-278          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-279          [-1, 14, 14, 384]             768\n",
            "          Linear-280         [-1, 14, 14, 1536]         591,360\n",
            "            GELU-281         [-1, 14, 14, 1536]               0\n",
            "         Dropout-282         [-1, 14, 14, 1536]               0\n",
            "          Linear-283          [-1, 14, 14, 384]         590,208\n",
            "         Dropout-284          [-1, 14, 14, 384]               0\n",
            "             Mlp-285          [-1, 14, 14, 384]               0\n",
            "        Identity-286          [-1, 14, 14, 384]               0\n",
            "      CycleBlock-287          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-288          [-1, 14, 14, 384]             768\n",
            "         CycleFC-289          [-1, 384, 14, 14]         147,840\n",
            "         CycleFC-290          [-1, 384, 14, 14]         147,840\n",
            "          Linear-291          [-1, 14, 14, 384]         147,456\n",
            "          Linear-292                   [-1, 96]          36,960\n",
            "            GELU-293                   [-1, 96]               0\n",
            "         Dropout-294                   [-1, 96]               0\n",
            "          Linear-295                 [-1, 1152]         111,744\n",
            "         Dropout-296                 [-1, 1152]               0\n",
            "             Mlp-297                 [-1, 1152]               0\n",
            "          Linear-298          [-1, 14, 14, 384]         147,840\n",
            "         Dropout-299          [-1, 14, 14, 384]               0\n",
            "        CycleMLP-300          [-1, 14, 14, 384]               0\n",
            "        Identity-301          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-302          [-1, 14, 14, 384]             768\n",
            "          Linear-303         [-1, 14, 14, 1536]         591,360\n",
            "            GELU-304         [-1, 14, 14, 1536]               0\n",
            "         Dropout-305         [-1, 14, 14, 1536]               0\n",
            "          Linear-306          [-1, 14, 14, 384]         590,208\n",
            "         Dropout-307          [-1, 14, 14, 384]               0\n",
            "             Mlp-308          [-1, 14, 14, 384]               0\n",
            "        Identity-309          [-1, 14, 14, 384]               0\n",
            "      CycleBlock-310          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-311          [-1, 14, 14, 384]             768\n",
            "         CycleFC-312          [-1, 384, 14, 14]         147,840\n",
            "         CycleFC-313          [-1, 384, 14, 14]         147,840\n",
            "          Linear-314          [-1, 14, 14, 384]         147,456\n",
            "          Linear-315                   [-1, 96]          36,960\n",
            "            GELU-316                   [-1, 96]               0\n",
            "         Dropout-317                   [-1, 96]               0\n",
            "          Linear-318                 [-1, 1152]         111,744\n",
            "         Dropout-319                 [-1, 1152]               0\n",
            "             Mlp-320                 [-1, 1152]               0\n",
            "          Linear-321          [-1, 14, 14, 384]         147,840\n",
            "         Dropout-322          [-1, 14, 14, 384]               0\n",
            "        CycleMLP-323          [-1, 14, 14, 384]               0\n",
            "        Identity-324          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-325          [-1, 14, 14, 384]             768\n",
            "          Linear-326         [-1, 14, 14, 1536]         591,360\n",
            "            GELU-327         [-1, 14, 14, 1536]               0\n",
            "         Dropout-328         [-1, 14, 14, 1536]               0\n",
            "          Linear-329          [-1, 14, 14, 384]         590,208\n",
            "         Dropout-330          [-1, 14, 14, 384]               0\n",
            "             Mlp-331          [-1, 14, 14, 384]               0\n",
            "        Identity-332          [-1, 14, 14, 384]               0\n",
            "      CycleBlock-333          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-334          [-1, 14, 14, 384]             768\n",
            "         CycleFC-335          [-1, 384, 14, 14]         147,840\n",
            "         CycleFC-336          [-1, 384, 14, 14]         147,840\n",
            "          Linear-337          [-1, 14, 14, 384]         147,456\n",
            "          Linear-338                   [-1, 96]          36,960\n",
            "            GELU-339                   [-1, 96]               0\n",
            "         Dropout-340                   [-1, 96]               0\n",
            "          Linear-341                 [-1, 1152]         111,744\n",
            "         Dropout-342                 [-1, 1152]               0\n",
            "             Mlp-343                 [-1, 1152]               0\n",
            "          Linear-344          [-1, 14, 14, 384]         147,840\n",
            "         Dropout-345          [-1, 14, 14, 384]               0\n",
            "        CycleMLP-346          [-1, 14, 14, 384]               0\n",
            "        Identity-347          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-348          [-1, 14, 14, 384]             768\n",
            "          Linear-349         [-1, 14, 14, 1536]         591,360\n",
            "            GELU-350         [-1, 14, 14, 1536]               0\n",
            "         Dropout-351         [-1, 14, 14, 1536]               0\n",
            "          Linear-352          [-1, 14, 14, 384]         590,208\n",
            "         Dropout-353          [-1, 14, 14, 384]               0\n",
            "             Mlp-354          [-1, 14, 14, 384]               0\n",
            "        Identity-355          [-1, 14, 14, 384]               0\n",
            "      CycleBlock-356          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-357          [-1, 14, 14, 384]             768\n",
            "         CycleFC-358          [-1, 384, 14, 14]         147,840\n",
            "         CycleFC-359          [-1, 384, 14, 14]         147,840\n",
            "          Linear-360          [-1, 14, 14, 384]         147,456\n",
            "          Linear-361                   [-1, 96]          36,960\n",
            "            GELU-362                   [-1, 96]               0\n",
            "         Dropout-363                   [-1, 96]               0\n",
            "          Linear-364                 [-1, 1152]         111,744\n",
            "         Dropout-365                 [-1, 1152]               0\n",
            "             Mlp-366                 [-1, 1152]               0\n",
            "          Linear-367          [-1, 14, 14, 384]         147,840\n",
            "         Dropout-368          [-1, 14, 14, 384]               0\n",
            "        CycleMLP-369          [-1, 14, 14, 384]               0\n",
            "        Identity-370          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-371          [-1, 14, 14, 384]             768\n",
            "          Linear-372         [-1, 14, 14, 1536]         591,360\n",
            "            GELU-373         [-1, 14, 14, 1536]               0\n",
            "         Dropout-374         [-1, 14, 14, 1536]               0\n",
            "          Linear-375          [-1, 14, 14, 384]         590,208\n",
            "         Dropout-376          [-1, 14, 14, 384]               0\n",
            "             Mlp-377          [-1, 14, 14, 384]               0\n",
            "        Identity-378          [-1, 14, 14, 384]               0\n",
            "      CycleBlock-379          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-380          [-1, 14, 14, 384]             768\n",
            "         CycleFC-381          [-1, 384, 14, 14]         147,840\n",
            "         CycleFC-382          [-1, 384, 14, 14]         147,840\n",
            "          Linear-383          [-1, 14, 14, 384]         147,456\n",
            "          Linear-384                   [-1, 96]          36,960\n",
            "            GELU-385                   [-1, 96]               0\n",
            "         Dropout-386                   [-1, 96]               0\n",
            "          Linear-387                 [-1, 1152]         111,744\n",
            "         Dropout-388                 [-1, 1152]               0\n",
            "             Mlp-389                 [-1, 1152]               0\n",
            "          Linear-390          [-1, 14, 14, 384]         147,840\n",
            "         Dropout-391          [-1, 14, 14, 384]               0\n",
            "        CycleMLP-392          [-1, 14, 14, 384]               0\n",
            "        Identity-393          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-394          [-1, 14, 14, 384]             768\n",
            "          Linear-395         [-1, 14, 14, 1536]         591,360\n",
            "            GELU-396         [-1, 14, 14, 1536]               0\n",
            "         Dropout-397         [-1, 14, 14, 1536]               0\n",
            "          Linear-398          [-1, 14, 14, 384]         590,208\n",
            "         Dropout-399          [-1, 14, 14, 384]               0\n",
            "             Mlp-400          [-1, 14, 14, 384]               0\n",
            "        Identity-401          [-1, 14, 14, 384]               0\n",
            "      CycleBlock-402          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-403          [-1, 14, 14, 384]             768\n",
            "         CycleFC-404          [-1, 384, 14, 14]         147,840\n",
            "         CycleFC-405          [-1, 384, 14, 14]         147,840\n",
            "          Linear-406          [-1, 14, 14, 384]         147,456\n",
            "          Linear-407                   [-1, 96]          36,960\n",
            "            GELU-408                   [-1, 96]               0\n",
            "         Dropout-409                   [-1, 96]               0\n",
            "          Linear-410                 [-1, 1152]         111,744\n",
            "         Dropout-411                 [-1, 1152]               0\n",
            "             Mlp-412                 [-1, 1152]               0\n",
            "          Linear-413          [-1, 14, 14, 384]         147,840\n",
            "         Dropout-414          [-1, 14, 14, 384]               0\n",
            "        CycleMLP-415          [-1, 14, 14, 384]               0\n",
            "        Identity-416          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-417          [-1, 14, 14, 384]             768\n",
            "          Linear-418         [-1, 14, 14, 1536]         591,360\n",
            "            GELU-419         [-1, 14, 14, 1536]               0\n",
            "         Dropout-420         [-1, 14, 14, 1536]               0\n",
            "          Linear-421          [-1, 14, 14, 384]         590,208\n",
            "         Dropout-422          [-1, 14, 14, 384]               0\n",
            "             Mlp-423          [-1, 14, 14, 384]               0\n",
            "        Identity-424          [-1, 14, 14, 384]               0\n",
            "      CycleBlock-425          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-426          [-1, 14, 14, 384]             768\n",
            "         CycleFC-427          [-1, 384, 14, 14]         147,840\n",
            "         CycleFC-428          [-1, 384, 14, 14]         147,840\n",
            "          Linear-429          [-1, 14, 14, 384]         147,456\n",
            "          Linear-430                   [-1, 96]          36,960\n",
            "            GELU-431                   [-1, 96]               0\n",
            "         Dropout-432                   [-1, 96]               0\n",
            "          Linear-433                 [-1, 1152]         111,744\n",
            "         Dropout-434                 [-1, 1152]               0\n",
            "             Mlp-435                 [-1, 1152]               0\n",
            "          Linear-436          [-1, 14, 14, 384]         147,840\n",
            "         Dropout-437          [-1, 14, 14, 384]               0\n",
            "        CycleMLP-438          [-1, 14, 14, 384]               0\n",
            "        Identity-439          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-440          [-1, 14, 14, 384]             768\n",
            "          Linear-441         [-1, 14, 14, 1536]         591,360\n",
            "            GELU-442         [-1, 14, 14, 1536]               0\n",
            "         Dropout-443         [-1, 14, 14, 1536]               0\n",
            "          Linear-444          [-1, 14, 14, 384]         590,208\n",
            "         Dropout-445          [-1, 14, 14, 384]               0\n",
            "             Mlp-446          [-1, 14, 14, 384]               0\n",
            "        Identity-447          [-1, 14, 14, 384]               0\n",
            "      CycleBlock-448          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-449          [-1, 14, 14, 384]             768\n",
            "         CycleFC-450          [-1, 384, 14, 14]         147,840\n",
            "         CycleFC-451          [-1, 384, 14, 14]         147,840\n",
            "          Linear-452          [-1, 14, 14, 384]         147,456\n",
            "          Linear-453                   [-1, 96]          36,960\n",
            "            GELU-454                   [-1, 96]               0\n",
            "         Dropout-455                   [-1, 96]               0\n",
            "          Linear-456                 [-1, 1152]         111,744\n",
            "         Dropout-457                 [-1, 1152]               0\n",
            "             Mlp-458                 [-1, 1152]               0\n",
            "          Linear-459          [-1, 14, 14, 384]         147,840\n",
            "         Dropout-460          [-1, 14, 14, 384]               0\n",
            "        CycleMLP-461          [-1, 14, 14, 384]               0\n",
            "        Identity-462          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-463          [-1, 14, 14, 384]             768\n",
            "          Linear-464         [-1, 14, 14, 1536]         591,360\n",
            "            GELU-465         [-1, 14, 14, 1536]               0\n",
            "         Dropout-466         [-1, 14, 14, 1536]               0\n",
            "          Linear-467          [-1, 14, 14, 384]         590,208\n",
            "         Dropout-468          [-1, 14, 14, 384]               0\n",
            "             Mlp-469          [-1, 14, 14, 384]               0\n",
            "        Identity-470          [-1, 14, 14, 384]               0\n",
            "      CycleBlock-471          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-472          [-1, 14, 14, 384]             768\n",
            "         CycleFC-473          [-1, 384, 14, 14]         147,840\n",
            "         CycleFC-474          [-1, 384, 14, 14]         147,840\n",
            "          Linear-475          [-1, 14, 14, 384]         147,456\n",
            "          Linear-476                   [-1, 96]          36,960\n",
            "            GELU-477                   [-1, 96]               0\n",
            "         Dropout-478                   [-1, 96]               0\n",
            "          Linear-479                 [-1, 1152]         111,744\n",
            "         Dropout-480                 [-1, 1152]               0\n",
            "             Mlp-481                 [-1, 1152]               0\n",
            "          Linear-482          [-1, 14, 14, 384]         147,840\n",
            "         Dropout-483          [-1, 14, 14, 384]               0\n",
            "        CycleMLP-484          [-1, 14, 14, 384]               0\n",
            "        Identity-485          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-486          [-1, 14, 14, 384]             768\n",
            "          Linear-487         [-1, 14, 14, 1536]         591,360\n",
            "            GELU-488         [-1, 14, 14, 1536]               0\n",
            "         Dropout-489         [-1, 14, 14, 1536]               0\n",
            "          Linear-490          [-1, 14, 14, 384]         590,208\n",
            "         Dropout-491          [-1, 14, 14, 384]               0\n",
            "             Mlp-492          [-1, 14, 14, 384]               0\n",
            "        Identity-493          [-1, 14, 14, 384]               0\n",
            "      CycleBlock-494          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-495          [-1, 14, 14, 384]             768\n",
            "         CycleFC-496          [-1, 384, 14, 14]         147,840\n",
            "         CycleFC-497          [-1, 384, 14, 14]         147,840\n",
            "          Linear-498          [-1, 14, 14, 384]         147,456\n",
            "          Linear-499                   [-1, 96]          36,960\n",
            "            GELU-500                   [-1, 96]               0\n",
            "         Dropout-501                   [-1, 96]               0\n",
            "          Linear-502                 [-1, 1152]         111,744\n",
            "         Dropout-503                 [-1, 1152]               0\n",
            "             Mlp-504                 [-1, 1152]               0\n",
            "          Linear-505          [-1, 14, 14, 384]         147,840\n",
            "         Dropout-506          [-1, 14, 14, 384]               0\n",
            "        CycleMLP-507          [-1, 14, 14, 384]               0\n",
            "        Identity-508          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-509          [-1, 14, 14, 384]             768\n",
            "          Linear-510         [-1, 14, 14, 1536]         591,360\n",
            "            GELU-511         [-1, 14, 14, 1536]               0\n",
            "         Dropout-512         [-1, 14, 14, 1536]               0\n",
            "          Linear-513          [-1, 14, 14, 384]         590,208\n",
            "         Dropout-514          [-1, 14, 14, 384]               0\n",
            "             Mlp-515          [-1, 14, 14, 384]               0\n",
            "        Identity-516          [-1, 14, 14, 384]               0\n",
            "      CycleBlock-517          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-518          [-1, 14, 14, 384]             768\n",
            "         CycleFC-519          [-1, 384, 14, 14]         147,840\n",
            "         CycleFC-520          [-1, 384, 14, 14]         147,840\n",
            "          Linear-521          [-1, 14, 14, 384]         147,456\n",
            "          Linear-522                   [-1, 96]          36,960\n",
            "            GELU-523                   [-1, 96]               0\n",
            "         Dropout-524                   [-1, 96]               0\n",
            "          Linear-525                 [-1, 1152]         111,744\n",
            "         Dropout-526                 [-1, 1152]               0\n",
            "             Mlp-527                 [-1, 1152]               0\n",
            "          Linear-528          [-1, 14, 14, 384]         147,840\n",
            "         Dropout-529          [-1, 14, 14, 384]               0\n",
            "        CycleMLP-530          [-1, 14, 14, 384]               0\n",
            "        Identity-531          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-532          [-1, 14, 14, 384]             768\n",
            "          Linear-533         [-1, 14, 14, 1536]         591,360\n",
            "            GELU-534         [-1, 14, 14, 1536]               0\n",
            "         Dropout-535         [-1, 14, 14, 1536]               0\n",
            "          Linear-536          [-1, 14, 14, 384]         590,208\n",
            "         Dropout-537          [-1, 14, 14, 384]               0\n",
            "             Mlp-538          [-1, 14, 14, 384]               0\n",
            "        Identity-539          [-1, 14, 14, 384]               0\n",
            "      CycleBlock-540          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-541          [-1, 14, 14, 384]             768\n",
            "         CycleFC-542          [-1, 384, 14, 14]         147,840\n",
            "         CycleFC-543          [-1, 384, 14, 14]         147,840\n",
            "          Linear-544          [-1, 14, 14, 384]         147,456\n",
            "          Linear-545                   [-1, 96]          36,960\n",
            "            GELU-546                   [-1, 96]               0\n",
            "         Dropout-547                   [-1, 96]               0\n",
            "          Linear-548                 [-1, 1152]         111,744\n",
            "         Dropout-549                 [-1, 1152]               0\n",
            "             Mlp-550                 [-1, 1152]               0\n",
            "          Linear-551          [-1, 14, 14, 384]         147,840\n",
            "         Dropout-552          [-1, 14, 14, 384]               0\n",
            "        CycleMLP-553          [-1, 14, 14, 384]               0\n",
            "        Identity-554          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-555          [-1, 14, 14, 384]             768\n",
            "          Linear-556         [-1, 14, 14, 1536]         591,360\n",
            "            GELU-557         [-1, 14, 14, 1536]               0\n",
            "         Dropout-558         [-1, 14, 14, 1536]               0\n",
            "          Linear-559          [-1, 14, 14, 384]         590,208\n",
            "         Dropout-560          [-1, 14, 14, 384]               0\n",
            "             Mlp-561          [-1, 14, 14, 384]               0\n",
            "        Identity-562          [-1, 14, 14, 384]               0\n",
            "      CycleBlock-563          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-564          [-1, 14, 14, 384]             768\n",
            "         CycleFC-565          [-1, 384, 14, 14]         147,840\n",
            "         CycleFC-566          [-1, 384, 14, 14]         147,840\n",
            "          Linear-567          [-1, 14, 14, 384]         147,456\n",
            "          Linear-568                   [-1, 96]          36,960\n",
            "            GELU-569                   [-1, 96]               0\n",
            "         Dropout-570                   [-1, 96]               0\n",
            "          Linear-571                 [-1, 1152]         111,744\n",
            "         Dropout-572                 [-1, 1152]               0\n",
            "             Mlp-573                 [-1, 1152]               0\n",
            "          Linear-574          [-1, 14, 14, 384]         147,840\n",
            "         Dropout-575          [-1, 14, 14, 384]               0\n",
            "        CycleMLP-576          [-1, 14, 14, 384]               0\n",
            "        Identity-577          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-578          [-1, 14, 14, 384]             768\n",
            "          Linear-579         [-1, 14, 14, 1536]         591,360\n",
            "            GELU-580         [-1, 14, 14, 1536]               0\n",
            "         Dropout-581         [-1, 14, 14, 1536]               0\n",
            "          Linear-582          [-1, 14, 14, 384]         590,208\n",
            "         Dropout-583          [-1, 14, 14, 384]               0\n",
            "             Mlp-584          [-1, 14, 14, 384]               0\n",
            "        Identity-585          [-1, 14, 14, 384]               0\n",
            "      CycleBlock-586          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-587          [-1, 14, 14, 384]             768\n",
            "         CycleFC-588          [-1, 384, 14, 14]         147,840\n",
            "         CycleFC-589          [-1, 384, 14, 14]         147,840\n",
            "          Linear-590          [-1, 14, 14, 384]         147,456\n",
            "          Linear-591                   [-1, 96]          36,960\n",
            "            GELU-592                   [-1, 96]               0\n",
            "         Dropout-593                   [-1, 96]               0\n",
            "          Linear-594                 [-1, 1152]         111,744\n",
            "         Dropout-595                 [-1, 1152]               0\n",
            "             Mlp-596                 [-1, 1152]               0\n",
            "          Linear-597          [-1, 14, 14, 384]         147,840\n",
            "         Dropout-598          [-1, 14, 14, 384]               0\n",
            "        CycleMLP-599          [-1, 14, 14, 384]               0\n",
            "        Identity-600          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-601          [-1, 14, 14, 384]             768\n",
            "          Linear-602         [-1, 14, 14, 1536]         591,360\n",
            "            GELU-603         [-1, 14, 14, 1536]               0\n",
            "         Dropout-604         [-1, 14, 14, 1536]               0\n",
            "          Linear-605          [-1, 14, 14, 384]         590,208\n",
            "         Dropout-606          [-1, 14, 14, 384]               0\n",
            "             Mlp-607          [-1, 14, 14, 384]               0\n",
            "        Identity-608          [-1, 14, 14, 384]               0\n",
            "      CycleBlock-609          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-610          [-1, 14, 14, 384]             768\n",
            "         CycleFC-611          [-1, 384, 14, 14]         147,840\n",
            "         CycleFC-612          [-1, 384, 14, 14]         147,840\n",
            "          Linear-613          [-1, 14, 14, 384]         147,456\n",
            "          Linear-614                   [-1, 96]          36,960\n",
            "            GELU-615                   [-1, 96]               0\n",
            "         Dropout-616                   [-1, 96]               0\n",
            "          Linear-617                 [-1, 1152]         111,744\n",
            "         Dropout-618                 [-1, 1152]               0\n",
            "             Mlp-619                 [-1, 1152]               0\n",
            "          Linear-620          [-1, 14, 14, 384]         147,840\n",
            "         Dropout-621          [-1, 14, 14, 384]               0\n",
            "        CycleMLP-622          [-1, 14, 14, 384]               0\n",
            "        Identity-623          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-624          [-1, 14, 14, 384]             768\n",
            "          Linear-625         [-1, 14, 14, 1536]         591,360\n",
            "            GELU-626         [-1, 14, 14, 1536]               0\n",
            "         Dropout-627         [-1, 14, 14, 1536]               0\n",
            "          Linear-628          [-1, 14, 14, 384]         590,208\n",
            "         Dropout-629          [-1, 14, 14, 384]               0\n",
            "             Mlp-630          [-1, 14, 14, 384]               0\n",
            "        Identity-631          [-1, 14, 14, 384]               0\n",
            "      CycleBlock-632          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-633          [-1, 14, 14, 384]             768\n",
            "         CycleFC-634          [-1, 384, 14, 14]         147,840\n",
            "         CycleFC-635          [-1, 384, 14, 14]         147,840\n",
            "          Linear-636          [-1, 14, 14, 384]         147,456\n",
            "          Linear-637                   [-1, 96]          36,960\n",
            "            GELU-638                   [-1, 96]               0\n",
            "         Dropout-639                   [-1, 96]               0\n",
            "          Linear-640                 [-1, 1152]         111,744\n",
            "         Dropout-641                 [-1, 1152]               0\n",
            "             Mlp-642                 [-1, 1152]               0\n",
            "          Linear-643          [-1, 14, 14, 384]         147,840\n",
            "         Dropout-644          [-1, 14, 14, 384]               0\n",
            "        CycleMLP-645          [-1, 14, 14, 384]               0\n",
            "        Identity-646          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-647          [-1, 14, 14, 384]             768\n",
            "          Linear-648         [-1, 14, 14, 1536]         591,360\n",
            "            GELU-649         [-1, 14, 14, 1536]               0\n",
            "         Dropout-650         [-1, 14, 14, 1536]               0\n",
            "          Linear-651          [-1, 14, 14, 384]         590,208\n",
            "         Dropout-652          [-1, 14, 14, 384]               0\n",
            "             Mlp-653          [-1, 14, 14, 384]               0\n",
            "        Identity-654          [-1, 14, 14, 384]               0\n",
            "      CycleBlock-655          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-656          [-1, 14, 14, 384]             768\n",
            "         CycleFC-657          [-1, 384, 14, 14]         147,840\n",
            "         CycleFC-658          [-1, 384, 14, 14]         147,840\n",
            "          Linear-659          [-1, 14, 14, 384]         147,456\n",
            "          Linear-660                   [-1, 96]          36,960\n",
            "            GELU-661                   [-1, 96]               0\n",
            "         Dropout-662                   [-1, 96]               0\n",
            "          Linear-663                 [-1, 1152]         111,744\n",
            "         Dropout-664                 [-1, 1152]               0\n",
            "             Mlp-665                 [-1, 1152]               0\n",
            "          Linear-666          [-1, 14, 14, 384]         147,840\n",
            "         Dropout-667          [-1, 14, 14, 384]               0\n",
            "        CycleMLP-668          [-1, 14, 14, 384]               0\n",
            "        Identity-669          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-670          [-1, 14, 14, 384]             768\n",
            "          Linear-671         [-1, 14, 14, 1536]         591,360\n",
            "            GELU-672         [-1, 14, 14, 1536]               0\n",
            "         Dropout-673         [-1, 14, 14, 1536]               0\n",
            "          Linear-674          [-1, 14, 14, 384]         590,208\n",
            "         Dropout-675          [-1, 14, 14, 384]               0\n",
            "             Mlp-676          [-1, 14, 14, 384]               0\n",
            "        Identity-677          [-1, 14, 14, 384]               0\n",
            "      CycleBlock-678          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-679          [-1, 14, 14, 384]             768\n",
            "         CycleFC-680          [-1, 384, 14, 14]         147,840\n",
            "         CycleFC-681          [-1, 384, 14, 14]         147,840\n",
            "          Linear-682          [-1, 14, 14, 384]         147,456\n",
            "          Linear-683                   [-1, 96]          36,960\n",
            "            GELU-684                   [-1, 96]               0\n",
            "         Dropout-685                   [-1, 96]               0\n",
            "          Linear-686                 [-1, 1152]         111,744\n",
            "         Dropout-687                 [-1, 1152]               0\n",
            "             Mlp-688                 [-1, 1152]               0\n",
            "          Linear-689          [-1, 14, 14, 384]         147,840\n",
            "         Dropout-690          [-1, 14, 14, 384]               0\n",
            "        CycleMLP-691          [-1, 14, 14, 384]               0\n",
            "        Identity-692          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-693          [-1, 14, 14, 384]             768\n",
            "          Linear-694         [-1, 14, 14, 1536]         591,360\n",
            "            GELU-695         [-1, 14, 14, 1536]               0\n",
            "         Dropout-696         [-1, 14, 14, 1536]               0\n",
            "          Linear-697          [-1, 14, 14, 384]         590,208\n",
            "         Dropout-698          [-1, 14, 14, 384]               0\n",
            "             Mlp-699          [-1, 14, 14, 384]               0\n",
            "        Identity-700          [-1, 14, 14, 384]               0\n",
            "      CycleBlock-701          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-702          [-1, 14, 14, 384]             768\n",
            "         CycleFC-703          [-1, 384, 14, 14]         147,840\n",
            "         CycleFC-704          [-1, 384, 14, 14]         147,840\n",
            "          Linear-705          [-1, 14, 14, 384]         147,456\n",
            "          Linear-706                   [-1, 96]          36,960\n",
            "            GELU-707                   [-1, 96]               0\n",
            "         Dropout-708                   [-1, 96]               0\n",
            "          Linear-709                 [-1, 1152]         111,744\n",
            "         Dropout-710                 [-1, 1152]               0\n",
            "             Mlp-711                 [-1, 1152]               0\n",
            "          Linear-712          [-1, 14, 14, 384]         147,840\n",
            "         Dropout-713          [-1, 14, 14, 384]               0\n",
            "        CycleMLP-714          [-1, 14, 14, 384]               0\n",
            "        Identity-715          [-1, 14, 14, 384]               0\n",
            "       LayerNorm-716          [-1, 14, 14, 384]             768\n",
            "          Linear-717         [-1, 14, 14, 1536]         591,360\n",
            "            GELU-718         [-1, 14, 14, 1536]               0\n",
            "         Dropout-719         [-1, 14, 14, 1536]               0\n",
            "          Linear-720          [-1, 14, 14, 384]         590,208\n",
            "         Dropout-721          [-1, 14, 14, 384]               0\n",
            "             Mlp-722          [-1, 14, 14, 384]               0\n",
            "        Identity-723          [-1, 14, 14, 384]               0\n",
            "      CycleBlock-724          [-1, 14, 14, 384]               0\n",
            "            ReLU-725           [-1, 96, 28, 28]               0\n",
            "          Conv2d-726           [-1, 48, 14, 14]           4,608\n",
            "          Conv2d-727           [-1, 48, 14, 14]           4,608\n",
            "     BatchNorm2d-728           [-1, 96, 14, 14]             192\n",
            "FactorizedReduce-729           [-1, 96, 14, 14]               0\n",
            "            ReLU-730          [-1, 192, 28, 28]               0\n",
            "          Conv2d-731           [-1, 96, 14, 14]          18,432\n",
            "          Conv2d-732           [-1, 96, 14, 14]          18,432\n",
            "     BatchNorm2d-733          [-1, 192, 14, 14]             384\n",
            "FactorizedReduce-734          [-1, 192, 14, 14]               0\n",
            "          Conv2d-735            [-1, 768, 7, 7]       2,654,976\n",
            "      Downsample-736            [-1, 7, 7, 768]               0\n",
            "       LayerNorm-737            [-1, 7, 7, 768]           1,536\n",
            "         CycleFC-738            [-1, 768, 7, 7]         590,592\n",
            "         CycleFC-739            [-1, 768, 7, 7]         590,592\n",
            "          Linear-740            [-1, 7, 7, 768]         589,824\n",
            "          Linear-741                  [-1, 192]         147,648\n",
            "            GELU-742                  [-1, 192]               0\n",
            "         Dropout-743                  [-1, 192]               0\n",
            "          Linear-744                 [-1, 2304]         444,672\n",
            "         Dropout-745                 [-1, 2304]               0\n",
            "             Mlp-746                 [-1, 2304]               0\n",
            "          Linear-747            [-1, 7, 7, 768]         590,592\n",
            "         Dropout-748            [-1, 7, 7, 768]               0\n",
            "        CycleMLP-749            [-1, 7, 7, 768]               0\n",
            "        Identity-750            [-1, 7, 7, 768]               0\n",
            "       LayerNorm-751            [-1, 7, 7, 768]           1,536\n",
            "          Linear-752           [-1, 7, 7, 3072]       2,362,368\n",
            "            GELU-753           [-1, 7, 7, 3072]               0\n",
            "         Dropout-754           [-1, 7, 7, 3072]               0\n",
            "          Linear-755            [-1, 7, 7, 768]       2,360,064\n",
            "         Dropout-756            [-1, 7, 7, 768]               0\n",
            "             Mlp-757            [-1, 7, 7, 768]               0\n",
            "        Identity-758            [-1, 7, 7, 768]               0\n",
            "      CycleBlock-759            [-1, 7, 7, 768]               0\n",
            "       LayerNorm-760            [-1, 7, 7, 768]           1,536\n",
            "         CycleFC-761            [-1, 768, 7, 7]         590,592\n",
            "         CycleFC-762            [-1, 768, 7, 7]         590,592\n",
            "          Linear-763            [-1, 7, 7, 768]         589,824\n",
            "          Linear-764                  [-1, 192]         147,648\n",
            "            GELU-765                  [-1, 192]               0\n",
            "         Dropout-766                  [-1, 192]               0\n",
            "          Linear-767                 [-1, 2304]         444,672\n",
            "         Dropout-768                 [-1, 2304]               0\n",
            "             Mlp-769                 [-1, 2304]               0\n",
            "          Linear-770            [-1, 7, 7, 768]         590,592\n",
            "         Dropout-771            [-1, 7, 7, 768]               0\n",
            "        CycleMLP-772            [-1, 7, 7, 768]               0\n",
            "        Identity-773            [-1, 7, 7, 768]               0\n",
            "       LayerNorm-774            [-1, 7, 7, 768]           1,536\n",
            "          Linear-775           [-1, 7, 7, 3072]       2,362,368\n",
            "            GELU-776           [-1, 7, 7, 3072]               0\n",
            "         Dropout-777           [-1, 7, 7, 3072]               0\n",
            "          Linear-778            [-1, 7, 7, 768]       2,360,064\n",
            "         Dropout-779            [-1, 7, 7, 768]               0\n",
            "             Mlp-780            [-1, 7, 7, 768]               0\n",
            "        Identity-781            [-1, 7, 7, 768]               0\n",
            "      CycleBlock-782            [-1, 7, 7, 768]               0\n",
            "       LayerNorm-783            [-1, 7, 7, 768]           1,536\n",
            "         CycleFC-784            [-1, 768, 7, 7]         590,592\n",
            "         CycleFC-785            [-1, 768, 7, 7]         590,592\n",
            "          Linear-786            [-1, 7, 7, 768]         589,824\n",
            "          Linear-787                  [-1, 192]         147,648\n",
            "            GELU-788                  [-1, 192]               0\n",
            "         Dropout-789                  [-1, 192]               0\n",
            "          Linear-790                 [-1, 2304]         444,672\n",
            "         Dropout-791                 [-1, 2304]               0\n",
            "             Mlp-792                 [-1, 2304]               0\n",
            "          Linear-793            [-1, 7, 7, 768]         590,592\n",
            "         Dropout-794            [-1, 7, 7, 768]               0\n",
            "        CycleMLP-795            [-1, 7, 7, 768]               0\n",
            "        Identity-796            [-1, 7, 7, 768]               0\n",
            "       LayerNorm-797            [-1, 7, 7, 768]           1,536\n",
            "          Linear-798           [-1, 7, 7, 3072]       2,362,368\n",
            "            GELU-799           [-1, 7, 7, 3072]               0\n",
            "         Dropout-800           [-1, 7, 7, 3072]               0\n",
            "          Linear-801            [-1, 7, 7, 768]       2,360,064\n",
            "         Dropout-802            [-1, 7, 7, 768]               0\n",
            "             Mlp-803            [-1, 7, 7, 768]               0\n",
            "        Identity-804            [-1, 7, 7, 768]               0\n",
            "      CycleBlock-805            [-1, 7, 7, 768]               0\n",
            "            ReLU-806           [-1, 96, 14, 14]               0\n",
            "          Conv2d-807             [-1, 48, 7, 7]           4,608\n",
            "          Conv2d-808             [-1, 48, 7, 7]           4,608\n",
            "     BatchNorm2d-809             [-1, 96, 7, 7]             192\n",
            "FactorizedReduce-810             [-1, 96, 7, 7]               0\n",
            "            ReLU-811          [-1, 192, 14, 14]               0\n",
            "          Conv2d-812             [-1, 96, 7, 7]          18,432\n",
            "          Conv2d-813             [-1, 96, 7, 7]          18,432\n",
            "     BatchNorm2d-814            [-1, 192, 7, 7]             384\n",
            "FactorizedReduce-815            [-1, 192, 7, 7]               0\n",
            "            ReLU-816          [-1, 384, 14, 14]               0\n",
            "          Conv2d-817            [-1, 192, 7, 7]          73,728\n",
            "          Conv2d-818            [-1, 192, 7, 7]          73,728\n",
            "     BatchNorm2d-819            [-1, 384, 7, 7]             768\n",
            "FactorizedReduce-820            [-1, 384, 7, 7]               0\n",
            "       LayerNorm-821             [-1, 49, 1440]           2,880\n",
            "          Linear-822                 [-1, 1000]       1,441,000\n",
            "================================================================\n",
            "Total params: 76,671,568\n",
            "Trainable params: 76,671,568\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 698.19\n",
            "Params size (MB): 292.48\n",
            "Estimated Total Size (MB): 991.24\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "InoyGuVwPbnm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd967336-bdb3-4e21-fadf-0ee0a0eee01d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CycleNet(\n",
            "  (patch_embed): PatchEmbedOverlapping(\n",
            "    (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))\n",
            "    (norm): Identity()\n",
            "  )\n",
            "  (network): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): CycleBlock(\n",
            "        (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CycleMLP(\n",
            "          (mlp_c): Linear(in_features=96, out_features=96, bias=False)\n",
            "          (sfc_h): CycleFC(CycleFC(96, 96, kernel_size=(1, 3), stride=(1, 1)))\n",
            "          (sfc_w): CycleFC(CycleFC(96, 96, kernel_size=(3, 1), stride=(1, 1)))\n",
            "          (reweight): Mlp(\n",
            "            (fc1): Linear(in_features=96, out_features=24, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (fc2): Linear(in_features=24, out_features=288, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (proj): Linear(in_features=96, out_features=96, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (1): CycleBlock(\n",
            "        (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CycleMLP(\n",
            "          (mlp_c): Linear(in_features=96, out_features=96, bias=False)\n",
            "          (sfc_h): CycleFC(CycleFC(96, 96, kernel_size=(1, 3), stride=(1, 1)))\n",
            "          (sfc_w): CycleFC(CycleFC(96, 96, kernel_size=(3, 1), stride=(1, 1)))\n",
            "          (reweight): Mlp(\n",
            "            (fc1): Linear(in_features=96, out_features=24, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (fc2): Linear(in_features=24, out_features=288, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (proj): Linear(in_features=96, out_features=96, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (2): CycleBlock(\n",
            "        (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CycleMLP(\n",
            "          (mlp_c): Linear(in_features=96, out_features=96, bias=False)\n",
            "          (sfc_h): CycleFC(CycleFC(96, 96, kernel_size=(1, 3), stride=(1, 1)))\n",
            "          (sfc_w): CycleFC(CycleFC(96, 96, kernel_size=(3, 1), stride=(1, 1)))\n",
            "          (reweight): Mlp(\n",
            "            (fc1): Linear(in_features=96, out_features=24, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (fc2): Linear(in_features=24, out_features=288, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (proj): Linear(in_features=96, out_features=96, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (1): Downsample(\n",
            "      (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "    (2): Sequential(\n",
            "      (0): CycleBlock(\n",
            "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CycleMLP(\n",
            "          (mlp_c): Linear(in_features=192, out_features=192, bias=False)\n",
            "          (sfc_h): CycleFC(CycleFC(192, 192, kernel_size=(1, 3), stride=(1, 1)))\n",
            "          (sfc_w): CycleFC(CycleFC(192, 192, kernel_size=(3, 1), stride=(1, 1)))\n",
            "          (reweight): Mlp(\n",
            "            (fc1): Linear(in_features=192, out_features=48, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (fc2): Linear(in_features=48, out_features=576, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (1): CycleBlock(\n",
            "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CycleMLP(\n",
            "          (mlp_c): Linear(in_features=192, out_features=192, bias=False)\n",
            "          (sfc_h): CycleFC(CycleFC(192, 192, kernel_size=(1, 3), stride=(1, 1)))\n",
            "          (sfc_w): CycleFC(CycleFC(192, 192, kernel_size=(3, 1), stride=(1, 1)))\n",
            "          (reweight): Mlp(\n",
            "            (fc1): Linear(in_features=192, out_features=48, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (fc2): Linear(in_features=48, out_features=576, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (2): CycleBlock(\n",
            "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CycleMLP(\n",
            "          (mlp_c): Linear(in_features=192, out_features=192, bias=False)\n",
            "          (sfc_h): CycleFC(CycleFC(192, 192, kernel_size=(1, 3), stride=(1, 1)))\n",
            "          (sfc_w): CycleFC(CycleFC(192, 192, kernel_size=(3, 1), stride=(1, 1)))\n",
            "          (reweight): Mlp(\n",
            "            (fc1): Linear(in_features=192, out_features=48, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (fc2): Linear(in_features=48, out_features=576, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (3): CycleBlock(\n",
            "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CycleMLP(\n",
            "          (mlp_c): Linear(in_features=192, out_features=192, bias=False)\n",
            "          (sfc_h): CycleFC(CycleFC(192, 192, kernel_size=(1, 3), stride=(1, 1)))\n",
            "          (sfc_w): CycleFC(CycleFC(192, 192, kernel_size=(3, 1), stride=(1, 1)))\n",
            "          (reweight): Mlp(\n",
            "            (fc1): Linear(in_features=192, out_features=48, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (fc2): Linear(in_features=48, out_features=576, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (3): Downsample(\n",
            "      (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "    (4): Sequential(\n",
            "      (0): CycleBlock(\n",
            "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CycleMLP(\n",
            "          (mlp_c): Linear(in_features=384, out_features=384, bias=False)\n",
            "          (sfc_h): CycleFC(CycleFC(384, 384, kernel_size=(1, 3), stride=(1, 1)))\n",
            "          (sfc_w): CycleFC(CycleFC(384, 384, kernel_size=(3, 1), stride=(1, 1)))\n",
            "          (reweight): Mlp(\n",
            "            (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (fc2): Linear(in_features=96, out_features=1152, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (1): CycleBlock(\n",
            "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CycleMLP(\n",
            "          (mlp_c): Linear(in_features=384, out_features=384, bias=False)\n",
            "          (sfc_h): CycleFC(CycleFC(384, 384, kernel_size=(1, 3), stride=(1, 1)))\n",
            "          (sfc_w): CycleFC(CycleFC(384, 384, kernel_size=(3, 1), stride=(1, 1)))\n",
            "          (reweight): Mlp(\n",
            "            (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (fc2): Linear(in_features=96, out_features=1152, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (2): CycleBlock(\n",
            "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CycleMLP(\n",
            "          (mlp_c): Linear(in_features=384, out_features=384, bias=False)\n",
            "          (sfc_h): CycleFC(CycleFC(384, 384, kernel_size=(1, 3), stride=(1, 1)))\n",
            "          (sfc_w): CycleFC(CycleFC(384, 384, kernel_size=(3, 1), stride=(1, 1)))\n",
            "          (reweight): Mlp(\n",
            "            (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (fc2): Linear(in_features=96, out_features=1152, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (3): CycleBlock(\n",
            "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CycleMLP(\n",
            "          (mlp_c): Linear(in_features=384, out_features=384, bias=False)\n",
            "          (sfc_h): CycleFC(CycleFC(384, 384, kernel_size=(1, 3), stride=(1, 1)))\n",
            "          (sfc_w): CycleFC(CycleFC(384, 384, kernel_size=(3, 1), stride=(1, 1)))\n",
            "          (reweight): Mlp(\n",
            "            (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (fc2): Linear(in_features=96, out_features=1152, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (4): CycleBlock(\n",
            "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CycleMLP(\n",
            "          (mlp_c): Linear(in_features=384, out_features=384, bias=False)\n",
            "          (sfc_h): CycleFC(CycleFC(384, 384, kernel_size=(1, 3), stride=(1, 1)))\n",
            "          (sfc_w): CycleFC(CycleFC(384, 384, kernel_size=(3, 1), stride=(1, 1)))\n",
            "          (reweight): Mlp(\n",
            "            (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (fc2): Linear(in_features=96, out_features=1152, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (5): CycleBlock(\n",
            "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CycleMLP(\n",
            "          (mlp_c): Linear(in_features=384, out_features=384, bias=False)\n",
            "          (sfc_h): CycleFC(CycleFC(384, 384, kernel_size=(1, 3), stride=(1, 1)))\n",
            "          (sfc_w): CycleFC(CycleFC(384, 384, kernel_size=(3, 1), stride=(1, 1)))\n",
            "          (reweight): Mlp(\n",
            "            (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (fc2): Linear(in_features=96, out_features=1152, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (6): CycleBlock(\n",
            "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CycleMLP(\n",
            "          (mlp_c): Linear(in_features=384, out_features=384, bias=False)\n",
            "          (sfc_h): CycleFC(CycleFC(384, 384, kernel_size=(1, 3), stride=(1, 1)))\n",
            "          (sfc_w): CycleFC(CycleFC(384, 384, kernel_size=(3, 1), stride=(1, 1)))\n",
            "          (reweight): Mlp(\n",
            "            (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (fc2): Linear(in_features=96, out_features=1152, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (7): CycleBlock(\n",
            "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CycleMLP(\n",
            "          (mlp_c): Linear(in_features=384, out_features=384, bias=False)\n",
            "          (sfc_h): CycleFC(CycleFC(384, 384, kernel_size=(1, 3), stride=(1, 1)))\n",
            "          (sfc_w): CycleFC(CycleFC(384, 384, kernel_size=(3, 1), stride=(1, 1)))\n",
            "          (reweight): Mlp(\n",
            "            (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (fc2): Linear(in_features=96, out_features=1152, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (8): CycleBlock(\n",
            "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CycleMLP(\n",
            "          (mlp_c): Linear(in_features=384, out_features=384, bias=False)\n",
            "          (sfc_h): CycleFC(CycleFC(384, 384, kernel_size=(1, 3), stride=(1, 1)))\n",
            "          (sfc_w): CycleFC(CycleFC(384, 384, kernel_size=(3, 1), stride=(1, 1)))\n",
            "          (reweight): Mlp(\n",
            "            (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (fc2): Linear(in_features=96, out_features=1152, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (9): CycleBlock(\n",
            "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CycleMLP(\n",
            "          (mlp_c): Linear(in_features=384, out_features=384, bias=False)\n",
            "          (sfc_h): CycleFC(CycleFC(384, 384, kernel_size=(1, 3), stride=(1, 1)))\n",
            "          (sfc_w): CycleFC(CycleFC(384, 384, kernel_size=(3, 1), stride=(1, 1)))\n",
            "          (reweight): Mlp(\n",
            "            (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (fc2): Linear(in_features=96, out_features=1152, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (10): CycleBlock(\n",
            "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CycleMLP(\n",
            "          (mlp_c): Linear(in_features=384, out_features=384, bias=False)\n",
            "          (sfc_h): CycleFC(CycleFC(384, 384, kernel_size=(1, 3), stride=(1, 1)))\n",
            "          (sfc_w): CycleFC(CycleFC(384, 384, kernel_size=(3, 1), stride=(1, 1)))\n",
            "          (reweight): Mlp(\n",
            "            (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (fc2): Linear(in_features=96, out_features=1152, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (11): CycleBlock(\n",
            "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CycleMLP(\n",
            "          (mlp_c): Linear(in_features=384, out_features=384, bias=False)\n",
            "          (sfc_h): CycleFC(CycleFC(384, 384, kernel_size=(1, 3), stride=(1, 1)))\n",
            "          (sfc_w): CycleFC(CycleFC(384, 384, kernel_size=(3, 1), stride=(1, 1)))\n",
            "          (reweight): Mlp(\n",
            "            (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (fc2): Linear(in_features=96, out_features=1152, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (12): CycleBlock(\n",
            "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CycleMLP(\n",
            "          (mlp_c): Linear(in_features=384, out_features=384, bias=False)\n",
            "          (sfc_h): CycleFC(CycleFC(384, 384, kernel_size=(1, 3), stride=(1, 1)))\n",
            "          (sfc_w): CycleFC(CycleFC(384, 384, kernel_size=(3, 1), stride=(1, 1)))\n",
            "          (reweight): Mlp(\n",
            "            (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (fc2): Linear(in_features=96, out_features=1152, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (13): CycleBlock(\n",
            "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CycleMLP(\n",
            "          (mlp_c): Linear(in_features=384, out_features=384, bias=False)\n",
            "          (sfc_h): CycleFC(CycleFC(384, 384, kernel_size=(1, 3), stride=(1, 1)))\n",
            "          (sfc_w): CycleFC(CycleFC(384, 384, kernel_size=(3, 1), stride=(1, 1)))\n",
            "          (reweight): Mlp(\n",
            "            (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (fc2): Linear(in_features=96, out_features=1152, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (14): CycleBlock(\n",
            "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CycleMLP(\n",
            "          (mlp_c): Linear(in_features=384, out_features=384, bias=False)\n",
            "          (sfc_h): CycleFC(CycleFC(384, 384, kernel_size=(1, 3), stride=(1, 1)))\n",
            "          (sfc_w): CycleFC(CycleFC(384, 384, kernel_size=(3, 1), stride=(1, 1)))\n",
            "          (reweight): Mlp(\n",
            "            (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (fc2): Linear(in_features=96, out_features=1152, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (15): CycleBlock(\n",
            "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CycleMLP(\n",
            "          (mlp_c): Linear(in_features=384, out_features=384, bias=False)\n",
            "          (sfc_h): CycleFC(CycleFC(384, 384, kernel_size=(1, 3), stride=(1, 1)))\n",
            "          (sfc_w): CycleFC(CycleFC(384, 384, kernel_size=(3, 1), stride=(1, 1)))\n",
            "          (reweight): Mlp(\n",
            "            (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (fc2): Linear(in_features=96, out_features=1152, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (16): CycleBlock(\n",
            "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CycleMLP(\n",
            "          (mlp_c): Linear(in_features=384, out_features=384, bias=False)\n",
            "          (sfc_h): CycleFC(CycleFC(384, 384, kernel_size=(1, 3), stride=(1, 1)))\n",
            "          (sfc_w): CycleFC(CycleFC(384, 384, kernel_size=(3, 1), stride=(1, 1)))\n",
            "          (reweight): Mlp(\n",
            "            (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (fc2): Linear(in_features=96, out_features=1152, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (17): CycleBlock(\n",
            "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CycleMLP(\n",
            "          (mlp_c): Linear(in_features=384, out_features=384, bias=False)\n",
            "          (sfc_h): CycleFC(CycleFC(384, 384, kernel_size=(1, 3), stride=(1, 1)))\n",
            "          (sfc_w): CycleFC(CycleFC(384, 384, kernel_size=(3, 1), stride=(1, 1)))\n",
            "          (reweight): Mlp(\n",
            "            (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (fc2): Linear(in_features=96, out_features=1152, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (18): CycleBlock(\n",
            "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CycleMLP(\n",
            "          (mlp_c): Linear(in_features=384, out_features=384, bias=False)\n",
            "          (sfc_h): CycleFC(CycleFC(384, 384, kernel_size=(1, 3), stride=(1, 1)))\n",
            "          (sfc_w): CycleFC(CycleFC(384, 384, kernel_size=(3, 1), stride=(1, 1)))\n",
            "          (reweight): Mlp(\n",
            "            (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (fc2): Linear(in_features=96, out_features=1152, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (19): CycleBlock(\n",
            "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CycleMLP(\n",
            "          (mlp_c): Linear(in_features=384, out_features=384, bias=False)\n",
            "          (sfc_h): CycleFC(CycleFC(384, 384, kernel_size=(1, 3), stride=(1, 1)))\n",
            "          (sfc_w): CycleFC(CycleFC(384, 384, kernel_size=(3, 1), stride=(1, 1)))\n",
            "          (reweight): Mlp(\n",
            "            (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (fc2): Linear(in_features=96, out_features=1152, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (20): CycleBlock(\n",
            "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CycleMLP(\n",
            "          (mlp_c): Linear(in_features=384, out_features=384, bias=False)\n",
            "          (sfc_h): CycleFC(CycleFC(384, 384, kernel_size=(1, 3), stride=(1, 1)))\n",
            "          (sfc_w): CycleFC(CycleFC(384, 384, kernel_size=(3, 1), stride=(1, 1)))\n",
            "          (reweight): Mlp(\n",
            "            (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (fc2): Linear(in_features=96, out_features=1152, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (21): CycleBlock(\n",
            "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CycleMLP(\n",
            "          (mlp_c): Linear(in_features=384, out_features=384, bias=False)\n",
            "          (sfc_h): CycleFC(CycleFC(384, 384, kernel_size=(1, 3), stride=(1, 1)))\n",
            "          (sfc_w): CycleFC(CycleFC(384, 384, kernel_size=(3, 1), stride=(1, 1)))\n",
            "          (reweight): Mlp(\n",
            "            (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (fc2): Linear(in_features=96, out_features=1152, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (22): CycleBlock(\n",
            "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CycleMLP(\n",
            "          (mlp_c): Linear(in_features=384, out_features=384, bias=False)\n",
            "          (sfc_h): CycleFC(CycleFC(384, 384, kernel_size=(1, 3), stride=(1, 1)))\n",
            "          (sfc_w): CycleFC(CycleFC(384, 384, kernel_size=(3, 1), stride=(1, 1)))\n",
            "          (reweight): Mlp(\n",
            "            (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (fc2): Linear(in_features=96, out_features=1152, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (23): CycleBlock(\n",
            "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CycleMLP(\n",
            "          (mlp_c): Linear(in_features=384, out_features=384, bias=False)\n",
            "          (sfc_h): CycleFC(CycleFC(384, 384, kernel_size=(1, 3), stride=(1, 1)))\n",
            "          (sfc_w): CycleFC(CycleFC(384, 384, kernel_size=(3, 1), stride=(1, 1)))\n",
            "          (reweight): Mlp(\n",
            "            (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (fc2): Linear(in_features=96, out_features=1152, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (5): Downsample(\n",
            "      (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "    (6): Sequential(\n",
            "      (0): CycleBlock(\n",
            "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CycleMLP(\n",
            "          (mlp_c): Linear(in_features=768, out_features=768, bias=False)\n",
            "          (sfc_h): CycleFC(CycleFC(768, 768, kernel_size=(1, 3), stride=(1, 1)))\n",
            "          (sfc_w): CycleFC(CycleFC(768, 768, kernel_size=(3, 1), stride=(1, 1)))\n",
            "          (reweight): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=192, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (fc2): Linear(in_features=192, out_features=2304, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (1): CycleBlock(\n",
            "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CycleMLP(\n",
            "          (mlp_c): Linear(in_features=768, out_features=768, bias=False)\n",
            "          (sfc_h): CycleFC(CycleFC(768, 768, kernel_size=(1, 3), stride=(1, 1)))\n",
            "          (sfc_w): CycleFC(CycleFC(768, 768, kernel_size=(3, 1), stride=(1, 1)))\n",
            "          (reweight): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=192, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (fc2): Linear(in_features=192, out_features=2304, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (2): CycleBlock(\n",
            "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CycleMLP(\n",
            "          (mlp_c): Linear(in_features=768, out_features=768, bias=False)\n",
            "          (sfc_h): CycleFC(CycleFC(768, 768, kernel_size=(1, 3), stride=(1, 1)))\n",
            "          (sfc_w): CycleFC(CycleFC(768, 768, kernel_size=(3, 1), stride=(1, 1)))\n",
            "          (reweight): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=192, bias=True)\n",
            "            (act): GELU(approximate=none)\n",
            "            (fc2): Linear(in_features=192, out_features=2304, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU(approximate=none)\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-model-summary"
      ],
      "metadata": {
        "id": "mjWMNMBRQYhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_model_summary\n",
        "\n",
        "print(pytorch_model_summary.summary(model, torch.empty(5,3,224,224), show_hierarchical=True))"
      ],
      "metadata": {
        "id": "fRv7sIQOQeUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchviz"
      ],
      "metadata": {
        "id": "JZ9Vq7vzSlui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchviz import make_dot\n",
        "\n",
        "x = torch.zeros(1, 3, 224, 224)\n",
        "make_dot(model(x), params=dict(list(model.named_parameters())))"
      ],
      "metadata": {
        "id": "Le-y1yylR0EC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hiddenlayer"
      ],
      "metadata": {
        "id": "WW144OS-RNZ8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}