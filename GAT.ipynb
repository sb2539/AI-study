{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAT.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPUqqPGBoRfVexIP1cQx9NH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sb2539/AI-study/blob/master/GAT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!pip install pyts\n",
        "!pip install einops"
      ],
      "metadata": {
        "id": "4Tzlr1kk3_z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEqiDRux18QC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.datasets import Planetoid\n",
        "import torch_geometric.transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pyts.image import RecurrencePlot, GramianAngularField, MarkovTransitionField\n",
        "from pyts.datasets import fetch_ucr_dataset, ucr_dataset_list\n",
        "from PIL import Image\n",
        "from skimage.transform import resize\n",
        "import os\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from einops.layers.torch import Rearrange, Reduce\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "in_features = 3 # 입력 feature (F)\n",
        "out_features = 3  # 노드의 feature 수 (F프라임)\n",
        "nb_nodes = 3   # 노드 개수\n",
        "\n",
        "# node에 대해서 W로 linear transform\n",
        "W = nn.Parameter(torch.zeros(size = (in_features, out_features )))\n",
        "nn.init.xavier_uniform_(W, gain=1.414)\n",
        "\n",
        "input = torch.rand(nb_nodes, in_features) #(3,3)\n",
        "wh = torch.mm(input, W)\n",
        "N = wh.size()[0]\n",
        "\n",
        "print(wh.shape ,\"\\n\", N) "
      ],
      "metadata": {
        "id": "CYsmBFUFdMzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = nn.Parameter(torch.zeros(size = (2*out_features, 1)))\n",
        "nn.init.xavier_uniform_(a.data, gain=1.414)\n",
        "print(a.shape)\n",
        "leakyrelu = nn.LeakyReLU(0.2)  # LeakyReLU"
      ],
      "metadata": {
        "id": "ByoEUZAuhOCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a_input = torch.cat([wh.repeat(1,N).view(N*N,-1),wh.repeat(N,1)]).view(N, -1, 2*out_features)"
      ],
      "metadata": {
        "id": "MxKIGqVohffl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([[1,2,3],\n",
        "                 [4,5,6],\n",
        "                 [7,8,9]])\n",
        "print(x,'\\n')\n",
        "y = x.repeat(1,N)\n",
        "print(y,'\\n')\n",
        "y_t = x.repeat(N,1)\n",
        "print(y_t,'\\n')\n",
        "z = y.view(N*N,-1)\n",
        "print(z,'\\n')\n",
        "c = torch.cat([z,y_t],dim=1)\n",
        "print(c,'\\n')\n",
        "b = c.view(N, -1, 2*out_features)\n",
        "print(b,'\\n')"
      ],
      "metadata": {
        "id": "j9cYw6c0kZn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e = leakyrelu(torch.matmul(a_input, a).squeeze(2))"
      ],
      "metadata": {
        "id": "KJTuOGlMdZda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Masked Attention\n",
        "adj = torch.randint(2, (3, 3))\n",
        "\n",
        "zero_vec  = -9e15*torch.ones_like(e)\n",
        "print(zero_vec.shape)\n",
        "print(zero_vec)\n",
        "print(adj.shape)\n",
        "print(adj)"
      ],
      "metadata": {
        "id": "6WDIBI7qhRmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention = torch.where(adj>0, e, zero_vec)\n",
        "print(adj,\"\\n\",e,\"\\n\",zero_vec)\n",
        "attention"
      ],
      "metadata": {
        "id": "M92-0PQ9kugJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention = F.softmax(attention, dim = 1)\n",
        "h_prime = torch.matmul(attention, wh)"
      ],
      "metadata": {
        "id": "bZM_4Stxk7sH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Z4hR-J8F7yd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name_data = 'Cora'# nodes : 2708, edges : 10,556, features = 1,433, classes : 7\n",
        "dataset = Planetoid(root= '/tmp/' + name_data, name = name_data)\n",
        "dataset.transform = T.NormalizeFeatures()"
      ],
      "metadata": {
        "id": "NivQn2X3s3EX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset[0]"
      ],
      "metadata": {
        "id": "pr7BcxSywIjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "jTEWInF22GaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.x[0].size()"
      ],
      "metadata": {
        "id": "-WMW_ul6wLbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.train_mask"
      ],
      "metadata": {
        "id": "QC8WsCE04X6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wht = torch.tensor([[1,2,3],\n",
        "                 [4,5,6],\n",
        "                 [7,8,9]])\n",
        "at = torch.tensor([[0,1,0],\n",
        "                 [1,0,1],\n",
        "                 [1,1,0]])\n",
        "t_prime = torch.matmul(at, wht)\n",
        "print(t_prime)"
      ],
      "metadata": {
        "id": "M5Y2gdjx8VcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beef = fetch_ucr_dataset('Beef', use_cache=False, data_home = None, return_X_y=False)\n",
        "beef_train = beef.data_train\n",
        "beef_target_train = beef.target_train\n",
        "beef_test = beef.data_test\n",
        "beef_target_test = beef.target_test"
      ],
      "metadata": {
        "id": "fEsVVZulzVMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_y = pd.DataFrame(data = beef_target_train)\n",
        "df_x = pd.DataFrame(data = beef_train)\n",
        "df_x = df_x.transpose()\n",
        "df_x.shape"
      ],
      "metadata": {
        "id": "jTUWxbJ82c6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GATLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
        "        super(GATLayer, self).__init__()\n",
        "        self.dropout       = dropout        # drop prob = 0.6\n",
        "        self.in_features   = in_features    # \n",
        "        self.out_features  = out_features   # \n",
        "        self.alpha         = alpha          # LeakyReLU with negative input slope, alpha = 0.2\n",
        "        self.concat        = concat         # conacat = True for all layers except the output layer.\n",
        "\n",
        "        \n",
        "        # Xavier Initialization of Weights\n",
        "        # Alternatively use weights_init to apply weights of choice \n",
        "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
        "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
        "        \n",
        "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
        "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
        "        \n",
        "        # LeakyReLU\n",
        "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        # Linear Transformation\n",
        "        h = torch.mm(input, self.W) # matrix multiplication\n",
        "        N = h.size()[0]\n",
        "        print(N)\n",
        "\n",
        "        # Attention Mechanism\n",
        "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
        "        e       = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
        "\n",
        "        # Masked Attention\n",
        "        zero_vec  = -9e15*torch.ones_like(e)\n",
        "        attention = torch.where(adj > 0, e, zero_vec)\n",
        "        \n",
        "        attention = F.softmax(attention, dim=1)\n",
        "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
        "        h_prime   = torch.matmul(attention, h)\n",
        "\n",
        "        if self.concat:\n",
        "            return F.elu(h_prime)\n",
        "        else:\n",
        "            return h_prime"
      ],
      "metadata": {
        "id": "auIvREZIzR7s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}